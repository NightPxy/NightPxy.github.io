---
layout: post
title:  "机器学习-线性模型"
date:   2019-10-10 13:31:01 +0800
categories: jvm
tag: [ml]
---

* content
{:toc}


# 线性模型  

## 基本形式  

线性模型是试图学得一个 通过属性的线性集合来进行预测的函数  

线性模型的优势  
* 线性模型形式简单,易于建模,却蕴含着一些重要的机器学习思想,许多功能更为强大的非线性模型,都可以在线性模型的基础上通过引入层级结构和高维映射而得  
* 线性模型有非常优异的可解释性  
例如 f好瓜(x) = 0.2 * x色泽 + 0.5 * x根蒂 + 0.3 * x敲声 + 1,则意味着通过色泽,根蒂和敲声进行好瓜评判时,根蒂最重要,而敲声比色泽重要  

# 线性回归  

在只有一个变量的情况下,线性回归可以用 y=ax+b的方程来表示  
n元线性回归同理 h(x)= z + a*x1 + b*x2 + ... + n* xn

线性回归的目的是,求解出合适的方程,在一元的情况下拟合出一条直线(多元的情况下拟合出一条平面或者曲面),可以近似的代表各个数据样本的值  
这其中的最好情况,就是一个距离各个样本点都很近的直线(或曲面)  

求出这个直线(或曲面)的方法,一般有两种  
* 最小二乘法  
* 梯度下降法  


## 最小二乘法  
 
二乘的意思不是乘以二,而是指平方,也就是`最小二乘法`是一种求最小平方差的方法  
在`最小二乘法`的思维中,平方差越小代表越拟合,平方差最小就是最拟合了  

最拟合求解,已经转化为求解平方差函数的最小值求解了  

`最小二乘法`的本质是直接求`全局最优的闭式解(close form solution)`,即真正的全局最优解,这是`最小二乘法`的最原始含义,也称为`狭义最小二乘法法`  

## 梯度下降法  

`梯度下降法`的目标也是求平方差最小解,与`最小二乘法`的区别在于  
* `最小二乘法`是直接求解全局最优解  
* `梯度下降法`是迭代求取局部最优解,也就是随机选择一点,通过步长逐步迭代下滑求解  
所以  
* `最小二乘法`是真正的全局最优解,是最理想的情况  
但实际过程中,可能面对求解全局最优的计算消耗过于庞大或根本无法求出全局最优的情况,此时`最小二乘法`根本无法起效  
* `梯度下降法`能求取任意情况的解,并且可以通过步长与迭代来控制计算规模  
但相应的,`梯度下降法`的接有相当大的可能不是全局最优解  

https://www.cnblogs.com/huangyc/p/9801261.html
