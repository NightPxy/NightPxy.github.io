---
layout: post
title:  "Hadoop 压缩"
date:   2018-07-02 13:31:01 +0800
categories: hadoop
tag: hadoop
---

* content
{:toc}


## 

文件压缩的优点  

* 减少存储空间  
* 减少网络传输  

文件压缩的缺点  

* 消耗CPU的计算能力  

## 压缩方式的对比  

压缩方式的对比主要从 **压缩特性** 和 **压缩性能** 两个维度进行比较  

* 压缩特性  
压缩特性主要指是否内置,是否native等,但最重要的核心指标是 **是否可分割**  
压缩是否可分割,代表着是否可以并行读取每个文件块  
举个例子说:  
  一个可分割的N块压缩文件,将会启动N个Map来并行读取 
  一个不可分割的N块压缩文件,因为无法随机读取,所以只会以一个Map来读取整个文件,这是非常有影响的  
所以,**大文件尽可能选择可切分特性,如果使用不可切分,尽可能使用文件不足一块的小文件中**  
* 压缩性能  
压缩性能的核心指标是 **压缩比** 和 **压缩/解压速率**  
这两者往往是反比关系,即更好的压缩比,往往是以更慢的压缩/解压速度为代价  

各压缩方式的详细指标对比如下:  

**压缩特性**  

| 格式 | codec | 后缀| 切割 | native| 工具 | 内置 |
| --- | --- | ---| ---|---| ---|---|
| gzip | org.apache.hadoop.io.compress.GzipCodec | .gz |  否 | 是 |gzip |是|
| bzip2 | org.apache.hadoop.io.compress.BZip2Codec |.bz2|是|否|bzip2|是|
| lzo |com.hadoop.compression.lzo.LzopCodec|.lzo|条件是|是|lzop|否|
| snappy |org.apache.hadoop.io.compress.SnappyCodec|.snappy|否|是|无|否|
| LZ4 | org.apache.hadoop.io.compress.Lz4Codec | .lz4 | | 是 |无|否|

***压缩性能**  

|压缩格式|压缩比|压缩速率|解压速率|
| --- | --- | ---| ---|
|gzip|13.4%|21 MB/s|118 MB/s|
|lzo|20.5%|135 MB/s|410 MB/s|
|snappy|22.2%|172 MB/s|409 MB/s|
|bzip2|13.2%|2.4MB/s|9.5MB/s|

**详细分析**  

* **gzip**  
  gzip 有非常好的压缩比,但是压缩速度比较差,并且最最重要的是不支持切割
  所以gzip用处是比较小的,因为各方面的特点都不明显.追求压缩比的场景下,唯一比bzip2好的就是稍块的速度,但是会失去bzip2天然的可分割特性  

* **lzo**  
  如果要使用MapReduce计算,lzo应该是首选  
  lzo 有不错的压缩比和非常不错的加解压速度,并且可以是可切割的  
  lzo的劣势是切割必须依赖索引,但这是可以克服的,没有索引可以创建索引等等  

* **LZ4** 与 **snappy**  
这两者非常类似,都属于快速压缩,即拥有最快的压缩速度和可以接收的压缩比,都有共同的缺点是不支持分割  
这两者都适合特别追求压缩速度,或者目标文件本身就不大的场景   
这两者之间  LZ4是后期之秀,相比snappy压缩速度会更快一点  

* **bzip2**  
具有最好的压缩比,并且天然支持随机读,但是有最慢的压缩速度(并且慢的不止一点). 与lzo这种相比,压缩比可以提升大约1倍,但是压缩时间需要多10倍  
特定条件下使用,比如归档数据,只是单纯保存,很少再使用了  
  


###  压缩格式的选择  

* 与容器格式配合使用,特别是ORC或者Parquet,列式存储将相同列(列结构相同)存储在一起,可以非常大的提高压缩比  

* 对于大型文件,尽可能使用可切割压缩,否则会造成MR执行效率非常低下 
对于大型文件还有一个思路是在应用端先行切割,再对分块进行快速压缩.此时应用端切割时应考虑压缩之后的大小,力求将压缩之后的块接近HDFS块大小   

* 对于小型文件(文件不超过一个HDFS块),可以考虑使用快速压缩  


## MapReduce 中的压缩  

### Map输入    

如果文件是压缩的,那么会根据**文件扩展名**推断出相应的Codec,进行相应的解压读取  
如果是可分块压缩方式,就会启动多个Map分块读取,如果是不可分块压缩,就会启动一个Map读取整个文件  

###  Map输出  

Map的输出会写到磁盘并通过网络传输到Reduce,这里是一个适用压缩的场景  
这里适合快速压缩
* Map输出是溢写块,一般情况下本身就不大,适合使用快速压缩.  
* 可分割的压缩带来的随机读好处.reduce享受不了,因为最终一个分组只会有一个Reduce读取  

|属性|类型|默认值|描述|
|---|---|---|---|
|mapreduce.output.fileoutputformat.compress| |false||
|mapreduce.output.fileoutputformat.compress.codec||org.apache.hadoop.io.compress.DefaultCodec||

|属性|类型|默认值|描述|
|---|---|---|---|
|mapred.output.compress| |false||
|mapred.compress.map.output||||
|mapred.output.compression.codec||||
|mapred.output.compression.type||||

### Reduce 输出  

Reduce会将结果最终输出到HDFS,这里也是适用压缩场景  
Reduce输出的压缩要考虑具体场景  
* 如果是输出为小文件就可以考虑快速压缩  
* 如果是大文件,且该输出是另一个任务的输入就建议使用lzo并创建索引  
* 如果是不考虑时间和CPU消耗的归档数据,可以考虑用bzip2压缩  



### LZO 创建索引  

注意点:  
* 创建目标索引的前提是不会再改变了,如果文件后续还会被追加写入,创建索引没有意义 
* 只支持对TextFile进行压缩  

```shell
/hadoop jar hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer   xxx.lzo
```

```java
LzoIndexer lzoIndexer = new LzoIndexer(conf);
lzoIndexer.index(new Path("hdfs-lz4-file-path"));

```



问题
parquet + 压缩 怎么支持切割?  
如果是snappy,lz4就不能支持切割  
因为parquet本身的数据格式,也不能在应用程序按文件大小先行切割  

如果是lz4,对于parquet文件如何切割还没试,lz4自己说只支持textfile,但是hive中明显是可以这么用的,得试验下  

Hive中压缩还没开始试



### Parquet  

Parquet 是支持嵌套类型.代表天然支持复杂数据类型

Parquet是一种二进制存储文件,人无法直接阅读  

Parquet是一种列式存储方式,

Parquet 是一种带Schema的存储格式,意味着它是自解析的,文件同时包含MetaData和SourceData
一个Parquet是由一个Header以及多个Block,以及一个foot结尾,  
header存储的是一个4字节数字PAR1用来识别整个Parquet文件的格式,  

foot存储的是文件MetaData,包括格式版本信息,schema信息以及同header的PAR1等等  

Block块存储的是真正的数据.内部有个概念叫RowGroup  

每一个black内部都有一组RowGroup,他们是一组由Column chunk组成的列数据,每一个Column chunk内部又包含它具有的Page列表,每个page就包含来自相同列的值,

当写入Parquet文件时，它会自动基于column的类型适配一个合适的编码,这也是压缩比为什么这么高的原因,比如写入多个"aaa",实际是"aaa"=123,然后分别多次123,数据重复率越高,压缩比就会越夸张,配合列式存储的同构数据的高重复率,Parquet的压缩比是非常高的  

Parquet文件对于每个page支持标准的压缩算法比如支持Snappy,gzip以及LZO压缩格式  

读取一个Parquet文件时，需要完全读取Footer的meatadata，Parquet不需要类似SequenceFile之类的游标跳跃检索,而是直接定位到block的边界,因为所有block的边界都存储于footer的metadata中(因为metadata的写入是在所有blocks块写入完成之后的，所以metadata始终持有每个block的位置）









































参考博客
http://www.hainiubl.com/topics/26