---
layout: post
title:  "提纲"
date:   2018-09-01 13:31:01 +0800
categories: interview
tag: spark
---

* content
{:toc}


# Spark  

## RDD的分布式弹性数据集体现  

在我的理解中,RDD的弹性主要体现在对数据集的抽象封装上.也就是指RDD描述的是一个抽象数据集而不是物理数据集.在这种抽象层面,为RDD带来容错,分区变换,自动内存与磁盘的切换等等  
首先,Spark在RDD中抽象出了依赖关系.每一个RDD在产生时都会记录产生它的父RDD(或者是顶层数据源),以及如何从父RDD转换到自己的过程.这样在某一个RDD出现错误时,可以从父RDD在此计算从而重新恢复  
其次,RDD封装了数据的存储.使我们不关心数据本身是来自内存或者磁盘,从而使数据集本身可以在内存和磁盘中切换  
再次,RDD透传分区,而使我们在使用时可以不关心数据集本身的分布式分布情况  

## RDD的五大特性以及体现  

## 常用RDD简述  

## RDD的分区数计算    
RDD的分区数大致计算方式如下:
* 在通过本地数据集合创建的RDD中,分区数量等于spark.default.parallelism 默认情况下本地模式下等于核数,YARN模式下等于executor数量(或者2,谁大取谁)  
* 在HDFS之类的外部文件中,分区数等于文件块数,即一个文件块对应一个分区  
此时HDFS的文件块不能超过128M(spark.files.maxPartitions) ,另Spark内置最小分区为2,即如果文件只有一个块,也会是两个分区 ???源码验证 
* 在KafkaDirect中,分区数等于kafka的分区数,即一个kafka的分区对应一个RDD的分区 
* 在HBase上, 分区数等于HBase的Region数  

## RDD的缓存是立即进行吗,缓存级别有哪些以及如何选择  

首先RDD的缓存是Lazy的,也就是调用persist只是为RDD打上一个标记,并不会产生实际执行,缓存的执行必须通过Action来完成  
因为缓存的实际执行是在RDD的遍历方法中,首先通过这个标记,透过BlockManager去尝试读取存储.如果读取成功就会就此返回,如果读取失败才会调用RDD计算方法进行计算,并将计算结果再次写入.而RDD本身是Lazy的,触发RDD遍历方法的先决是执行Action.所以缓存也必须通过Action  

Spark提供的缓存级别有仅内存,内存且允许溢写到磁盘这三种,配合是否序列化6种,配合更多副本数更多.在选择上,官方的建议是如果仅内存没有问题那就仅内存最好,如果频繁发生GC或者OOM,可以尝试使用序列化内存,并且是kyio的序列化内存,可以减少缓存内存占用,当然代价是消耗更多的CPU资源来进行序列化和反序列化.除非是那种重新计算比从磁盘读取慢的多的情况,否则一般情况不考虑磁盘.

## BlockManager怎么管理硬盘和内存的  

Spark通过BlockManger,在Driver和Executor之间构建出一个小型的类似HDFS的分布式存储来进行管理  
首先,Spark抽象的最小存储单位为Block.一个Block等价于RDD一个分区,以Block为整体存储,不存在半块的情况.每一个Block都会有一个唯一ID  
其次,在Spark启动时会通过BlockManager分别在Driver启动BlockManagerMaster,在Executor启动BlockManagerSlaver,Master负责管理Block元数据并对外暴露元数据服务,Slave通过Netty在Master完成注册并维持心跳,并对外暴露本节点的数据传输服务.这样首先构建出类似NN和DN分布式存储通讯架构  

读取和写入这套分布式存储也与HDFS非常类似 

BlockManagerMaster类似NN,负责管理Block元数据.其中是通过三个HashMap来负责存储 
*  ExecutorId与BlockManagerId  
*  BlockManagerId与BlockManagerInfo(包括已存储块信息,目标传输服务引用等)  
*  BlockId与BlockManagerId集合(表示块被存储在哪些节点上,多个表示副本情况)

在读取时,首先通过getLocation方法,通过BlockId请求Master的元数据服务,获取Block的元数据,这些元数据包括存储方式(内存&磁盘),存储的Slaves位置以及副本数等等  
*  通过元数据可以感知Block是以内存还是磁盘存储,并同时对返回位置信息按照数据本地性依次请求.
*  如果是本地读取,通过getLocalValue读取.读取时会根据是内存还是磁盘切换MemoryStore或DiskStore来负责真正读取.MemoryStore的读取就是直接内存读取,因为Spark的内存存储载体是一个LinkHashMap.DiskStore则通过文件名计算出文件块的本地磁盘路径读取  
* 如果是远程读取则getRemoteValues方法,其通过元数据返回的位置请求目标BlockManagerSlaver暴露的数据传输服务拉取  

在写入时,也会请求Master的元数据服务来获取需要存储方式,存储节点以及副本数等等  
* 如果是内存写入,就会相当于写入LinkHashMap.此时会将所有的Block逐块写入,每写入之前会查看内存标量是否足够,如果足够则直接写入并提升内存标量,如果不够则会尝试扩容,直到超过当前Executor的存储内存则会弹出,以及设定存储方式决定是溢写到磁盘还是就此放弃  
* 如果是磁盘写入,就会根据文件名取两次Hash分别作为一级目录(16位UUID)和二级目录(64)并组装成本地磁盘路径,然后写入  
* 如果需要写入副本,则会联系副本目标所在的BlockManagerSlaver的传输服务写入  
* 全部写入完毕后,再次联系Master元数据服务告知写入完毕  

## RDD的宽依赖与窄依赖,有什么区别,Join是宽还是窄依赖  

宽依赖与窄依赖,是描述两个有血缘关系的RDD的分区之间的关系的描述  
如果父RDD的某一个分区全部进入子RDD的某一个分区,比如一对一或者多对一,就是窄依赖  
如果父RDD的某一个分区只能部分进入子RDD的某一个分区,就是宽依赖   

本质上,宽窄依赖其实对shuffle的描述.即事实上,宽窄依赖影响的是DAGScheduler绘制DAG图时是产生ResultStage还是ShuffleStage,并由此产生ResultTask还是ShuffleTask   

从执行效率来说,窄依赖有非常明显的优势,因为Spark的Stage才提交时会通过检查是有MissStage在执行而不断递归自旋,换句话说Spark的Stage是串行执行的,前一个执行完毕才会启动执行下一个.
这代表如果是纯窄依赖的ResultStage内部的Task将会并行执行,而宽依赖因为会被迫产生一个ShuffleStage,会先行执行ShuffleStage,并且在等待ShuffleStage所有Task全部执行完毕后,才会再提交ShuffleStage之后我们真正的FinalStage才会被提交执行.

## Spark是如何分解出DAG图的  

首先,绘制DAG的核心类是DAGScheduler  
绘制过程从Action开始,准确说是从每一个Action里的runJob方法开始  
* runJob会将action的算子记录,然后连同action所在的RDD一起包装为一个Job.这就是每一个Action产生一个Job  
* Job会被提交Spark内置一个消息总线中,并在submitJob中得到处理.  
submitJob处理首先为这个Job构建一个FinalStage(ResultStage)(每一个Job包含至少一个Stage),然后沿着Job提交的RDD的血缘关系逆流而上进行溯源,溯源过程中检查是否有shuffle,如果没有shuffle就会加入当前Stage,如果有shuffle,就会构造出一个ShuffleStage,然后继续溯源.在Stage内部的溯源过程中按照广度优先的策略,将RDD不断压入一个ArrayStack并最终在溯源完毕后栈出完成倒装  
这样最终遍历结果就是这样一个(画图)每一个Stage中包含多个RDD的StageGroups.
DAG绘制的最终结果就是这个StageGroup.其数据结构就呈现出有向无环图的特征.这就是大名鼎鼎的DAG图,有向是从一个Stage到另一个Stage,每一个Stage中一个RDD到另一个RDD的调用方向,无环是这个调用方向是单向,不存在循环引用.  

## Spark的DAG图是如何调度执行的  

DAG图本质是有Stages组成的StageGroup.而在调度执行这个StageGroup是一个一个Stage串行执行,也就是执行前首先判断是否有MissStage正在执行并不断自旋递归.  
而调度执行Stage的核心类时TaskScheduler.而整个调度执行过程,非常类似YARN  

* 提前架构类似YARN的调度通讯网络  
首先在SparkContext时,就会启动ScheduleBackEnd线程(driver中),类似YARN中ResourceManager,并在executor中同时启动ScheduleExecutorBackEnd,并通过RPC向driver注册自己并维持心跳,driver会持有executor的RPC引用来获得对executor的通讯能力.这样首先在driver和executor之间建立一个主从的类似YARN中ResourceManager和NodeManager的调度通讯体系  

* 在dagScheduler.submitMissStage中解析stage  
首先获取出RDD的分区数量,这是后面决定生成多少个Task(一个分区对应一个Task)  
其次是获取出RDD的位置信息,这是后面任务调度时数据本地化的依据  
然后是序列化stage的执行过程.所谓的执行过程,就是RDD的遍历函数的反射位置.所谓的分布式执行,就是提前传输jar到executor,并依次反射出这些RDD遍历函数来执行  
最后就是根据分区数量构建出Tasks,将分区,位置,序列化的执行过程的相关信息等包装为一个Task,如果是ResultStage,就会包装成ResultTask,如果是ShuffleStage就会包装ShuffleTask  
最终的结果是将Stage最终分解为一堆设定好的TaskSet提交给TaskScheduler.submitTask调度执行(用Set包装,表示在Spark的视角中,不在乎Task的执行顺序,事实上在分布式环境上受限目标机的线程调度情况,也不可能保证Task的执行顺序)  

* TaskScheduler.submitTask接受TaskSet   
TaskScheduler.submitTask会遍历TaskSet调度执行每一个Task  
在每一个Task的调度执行过程中  
首先会构建TaskSetManager的调度器(FIFO或者Fair调度器)来调度执行这批Task  
其次会通过SchedulerBackEnd.reviveOffers发生TaskSet,在makeOffers中挑选节点执行  

* SchedulerBackEnd.makeOffers挑选节点执行
首先会获取当前可用的executor列表,并遍历TaskSet,将每一个Task和当前所有可用的Executor包装成一个WordOffer
再对每一个WordOffer,也就是每一个Task和所有可用的Executor列表之间进行过滤选择.  
过滤选择方法是首先排除黑名单Executor,再按照executor和host排序逐个尝试去占内存和核,目标是期望选用最接近数据的executor执行Task.一旦占用成功就选择该Executor为执行节点
最后通过TaskScheduler.launchTasks提交这批WorkOffer.将Task信息和将要执行这个Task的Executor一起包装为一个TaskDescription,这个TaskDescription最后将含有TaskId,ExecutorId,序列化的执行逻辑以及相关的AddJars和AddFile  
最后这个TaskDescription会由TaskScheduleBackEnd根据ExecutorId发送给目标Executor  

* Executor接受执行  
Executor在收到TaskDescription之后,通过launchTask解析,然后包装成TaskRunner的Runable,并提交给自己的Executor执行  

* 执行完毕后,根据Task任务类型决定如何返回  
如果是ResultTask,就会通过RPC告知driver,并计算结果发回  
如果是ShuffleTask,就会将Shuffle结果存入自己的BlockManager,并将BlockId等存储信息发回driver

## Spark的内存管理  

这里只针对executor的内存管理简述.  
在executor中,整个JVM的内存被隐式的分为三大块.  
* 预留内存  
预留内存,固定300M,起预留作用  
因为本质上,没有任何办法可以控制对象在JVM中实际的内存分布,所以所谓的分块本质是一种上限标量,即在整个JVM内存的总大小上设定一个各块使用的上限来完成.本质上说,无论用户,计算还是存储的对象都是在JVM堆上的对象,并没有本质区别.这里可能是Spark为了避免一些临界问题,所以引入了预留内存,理论上不在各块之中,但实际可以被用户占用.  
需要注意的是,源码中明确指定了启动Spark内存至少是1.5倍(源码写死的)预留内存,即Spark启动的最小内存是450M  

* Spark内存   
Spark框架所使用的部分,由`spark.memory.fraction`控制,默认0.6,即(JVM内存-预留内存)*0.6为Spark使用的上限  
Spark使用的内存,再继续分为两大块  **执行内存** 和 **存储内存**  
存储内存为shuffle结果数据存储,缓存以及广播变量等所使用的内存  
执行内存为shuffle中间数据存储,连接以及排序等Spark内置环节计算所使用的内存  
其计算比例为 存储内存`spark.memory.storageFaction`控制,默认0.5,剩余为执行内存  
这其中存储内存和执行内存可以相互借用,规则如下:  
存储内存可以超出执行内存的阈值来进行存储,但当执行内存需要时可以立即释放多余存储  
执行内存可以超出存储内存的阈值设定0.5来进行计算,但可以为存储内存设定一个最小保留值,但存储需要时执行内存不还,而是等待执行结束  

简单来说,就是存储借用必还,执行借用不还.因为存储的占用是按块为单位,基于RDD容错机制,直接干掉无所谓,但执行内存的计算上下文非常复杂,所以正在执行的计算内存不予归还  


* 用户内存  
用户内存是指在用户使用的内存,包括用户在算子使用的一些临时数据或者数据结果等等  
比如mapPartition算子,将返回分区下全部数据列表,这个List数据结构将占有用户内存  

* 2G内存计算如下  
预留内存:300M  
Spark执行内存: (2048-300)*0.6*0.5 = 524   
Spark计算内存为: (2048-300)*0.6* (1-0.5)=524   
Spark用户内存为:(2048-300)*0.4=699  

## Spark的OOM一般有哪些,如何处理  

OOM即内存溢出.简单来说,就是JVM上的堆满了,GC清理掉不可用的对象之后还是分配不了内存,就会触发内存溢出  
Spark本质上是有driver和executor两种JVM,所以首先区分的是OOM是driver溢出还是executor溢出  

**driver溢出**  

* 首先要关注的是executor的结果返回接收  
一般情况下,不应该返回executor的计算结果,尽量在executor解决,这也更符合Spark设计的初衷,就算有返还结果,也应该是非常少量的,比如抽样100条这种  
如果涉及大结果返还,一个driver或面对N多的executor,很容易撑爆driver.这里就必须调整逻辑,尽量在executor完成逻辑  
* 其次是driver的内存是否过小  
driver将作为整个Spark作业的计算中心  
包括BlockManager的元数据存储,TaskScheduler的调用监控以及executor的元数据,StageSchedule的整个DAG分解图等等,所以承载的东西非常多,所以相对的需要将driver资源适当提高(NN和RM,也会要求是集群能达到的最好的配置不是)  

**executor溢出**  

* 首先应该为executor分配的内存大小  
这里的合适的标准在我的理解非常简单,就是一个executor的内存必须至少能保证在完整放下整个分区数据,并且还能有余力稍微做一点事情  
完整放下分区,与优化无关.因为这不是我们能代码优化解决的,再怎么说,算子总要用吧.  
在Spark中,相当多的算子在设计之初就是需要加载整个分区,比如mapPartition,这也是Spark高吃内存的原因之一,其底层就决定了必须能承载一个分区  
* executor必须保证容纳分区的同时还能有一点余力做事,但是受限集群资源,有时有不能做到这一点,也就是executor的资源有上限,也就是加资源不行,此时另一个办法就是减分区大小.  
减分区的大小非常简单,就是重分区.reparation,通过提升分区的数量来减少分区的大小.  
分区小了,也就能满足executor完整容纳的要求  
但是通过reparation减分区不是万能的,这里有涉及倾斜的问题    
* 倾斜问题  
倾斜问题,可以简单看作同Key问题.  
同Key倾斜非常容易出问题,可能引发shuffle溢出,shuffle不溢出也可能导致分区数据过大也产生溢出. 
因为分区的依据是Key,分区的本质是通过Key的Hash对分区数取模来落入分区,这有个问题是如果是同一个Key,无论怎么调整分区数量,都不会减少分区大小,因为同Key必然同分区  
如果有同Key倾斜,最简单的办法就是打散分区,比如为Key添加随机缀等,通过改变Key来打散分区  
注意打散时需要注意是否还需要再聚合,如果是不需要聚合怎么打散都无所谓,如果是需要聚合必须按照一定的格式打缀,这样通过打散之后的聚合结果再去缀进行二次聚合    
*  代码逻辑   
本质上,executor就是一个java应用,跑在JVM.没有任何区别,所以一些原来的代码优化也是可行的  
比如循环内的反复创建等等,比如map算子的循环new StringBuilder(线程不安全)等等    
* coalesce  
这是一个容易忽视的问题.  
假设原先有100个块,应该是100个task,但是因为其结果输出过少,为了防止小文件coalesce(10),结果内存爆了.  
coalesce的调小分区是窄依赖,不会产生shuffle.所以coalesce(10)设定RDD的分区数是10,会认为stage的分区是10,taskScheduler会因此认为应该使用10个task.原来100个task变成10个task,一个task读10块就爆了  
此时需要使用coalesce强制使用shuffle为true或者直接使用reparation来降分区,通过shuffle为打断stage,前面有100个task读,计算结构再交由10个task写出去  

coalesce(numPartitions: Int, shuffle: Boolean = false)
repartition=coalesce(numPartitions, shuffle = true)
