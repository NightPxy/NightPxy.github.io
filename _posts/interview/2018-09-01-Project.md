---
layout: post
title:  "Project"
date:   2018-09-01 13:31:01 +0800
categories: interview
tag: spark
---

* content
{:toc}


# 订单数据对账系统(离线ETL项目)   

## 项目背景  

采集第三方公司的订单数据,与公司订单完成自动化的对账并给出订单差异明细和统计报表  

重难点  
* 各种第三方公司采集源差异非常大且不可能改变  
数据文件包括Excel,CSV,TXT,RAR(Excel),RAR(CSV),RAR(TXT),Zip(Excel)......等等  
数据列组织方式千差万别  
提供方式有FTP,OS系统下载,服务接口下载,部分甚至有爬网页的可能  
* 数据延时,订单结果可能会跨天体现
* 全自动或可视化进行,尽可能减少人工干预  

## 架构  

* 数仓  
订单数据进入数仓,订单对账过程最终落地为数仓中以订单对账为主题的离线ETL计算  
数仓是整个对账系统的基石,核心主要体现在 数仓元数据 与 数据存储 两个部分 
* 作业调度(自研的小型分布式作业调度系统)  
ETL的采集,清洗,分析,输出将落地作业进行调度执行  
* 对账系统  

* 外围采集系统  
设计采集的主方式为FTP采集,将其他异构方式(人工下载,爬网页等)在外围采集系统中推入公司内FTP,之后再按照主FTP方式进入后续流程  
* WebUI  
提供对账任务的过程监控,重跑,差异查询,文件下载,报表等等  
主要服务对象为 技术人员 和 财务&运营人员 两类  

## Spark    

整个系统的技术核心是Spark,整体是围绕着SparkSQL打造的  

## 系统  

### 数仓  

数仓(元数据和存储)是近通用设计,与业务没有关系  
数仓的客户端使用体现为一个客户端Jar包  

#### 数据存储  

数仓数据存储基于 HDFS  
数仓元数据存储基于 MySQL  

#### 数仓元数据  

##### 概述  

数仓中核心是数据块(跟HDFS块没有关系)  
* 数据块是数仓使用中的最小单位  
* 数据块的本质是HDFS的一个目录  
* 数据块按照 `库.主题.数据块` 的形式组织  
库代表着一个HDFS集群  
主题代表着HDFS下一个独立的目录  
数据块是主题目录下的一个独立子目录  
* 数据块元数据将会自描述`文件格式`,`压缩方式`等使用方式  
* 数据块元数据将会自描述`记录数`,`总字节数`,`进入时间`,`使用(读取次数)`,`最近一次使用时间`等统计信息  
* 数据块有其血缘关系,将会描述该数据块是由哪些数据块而来 
血缘关系基于数据块产生,每一个血缘关系的展示是由一个数据块为中心展示其流出和流入   

##### 库  

|列|描述|
|--|--|
|db_id|数仓库ID|
|name|数仓库名称|
|url|数仓地址(HDFS连接地址)|
|user|数仓用户名(HDFS用户名)|

##### 主题   
|列|描述|
|--|--|
|topic_id|数据主题ID|
|db_id|数据主题所在库|
|name|数据主题名称|
|path|数据主题的HDFS-PATH|

##### 数据块  

|列|描述|
|--|--|
|data_id|数据块ID|
|db_id|数据块所在库|
|topic_id|数据块归属主题|
|name|数据块名称|
|format|数据块格式(基于Spark:Parquet,JSON,CSV)|
|compress|数据块压缩方式(基于Spark:None,Snappy)|
|total_count|数据块下的记录数|
|total_size|数据块下的总字节数|
|in_time|数据块的更新时间|
|use_count|数据块的使用次数(每读取一次数据块元数据加一)|
|use_lasttime|数据块的最后使用时间(最后一次读取数据块元数据时间)|

##### 血缘  

|列|描述|
|--|--|
|data_id|数据块ID|
|parent_id|该数据块的父源数据块ID|


### 小型分布式作业调度  

#### 概述

这个作业调度系统是自研的  
试过Azikaban等开源作业调度系统,有一个相对不能适用的问题在于,必须能够自由的控制作业执行,重跑等等  
* 这种自由必须体现在我们自己的对账系统UI中,即可以在我们的UI中自由的触发多级的任务重跑  
* 这种自由  


多级任务依赖

#### 设计  

##### 分布式设计 

调度系统是一个分布式的主从架构(但保持程序节点一致,仅仅是主从节点各自启动不同的线程)  
* 主从节点的通讯依赖RabbitMQ  
(没有使用Netty架构主从节点通讯网,维护Netty非常繁琐,还有心跳等等,而且在这个作业调度系统主从间不需要端到端的定点通讯,只需要主节点广播出一个执行命令即可,不关心是谁执行这个命令)  
* 从节点扩容与HA  
从节点基于MQ订阅消息,直接扩容即可,HA类似  
* 主节点扩容与HA  
主节点为单节点,不支持扩容与HA  
(可以考虑做`去中心化`:引入ZK,做主节点的动态选举.这样主节点将在整个节点中流动,只要还存在两个以上节点就可以正常运行从而实现主节点的HA)  

主节点  
* 根据触发时间触发任务的执行,并通过RabbitMQ发送任务执行命令  
* 更新计算触发后的下次触发时间  
* 对外提供Rest服务,提供任务的编排制作,重跑,步骤完毕通知等  

从节点  
* 订阅任务执行消息,通过消息中命令Id从元数据中还原,执行命令(命令的本质是执行一段提前写好的shell脚本)  
* 从节点可以通过配置设置自己的最大同时消费线程数,来保护自己防止因为命令过度崩溃  

##### 调度执行设计  

作业调度的本质是一个基于时间有序的命令队列执行  
落地实现方面我做的比较low,是基于DB轮询一个基于命令时间排序的数据表做的(只会有主节点的一个线程的间隔轮询,在性能方面是没有问题的)  

作业的执行上的最小单位是一个步骤  
即每一个命令执行的是任务的某一个步骤,


##### 元数据设计  

作业系统有自己独立的元数据  
一个作业本身是有一个主任务与步骤流(多个有流向的步骤)组成  

* 任务  
每个任务有独立的任务ID,下次触发时间,触发规则等   
* 步骤  
步骤中存储一段由用户输入的shell命令  
每个步骤都会有一个下一步骤ID,由此组成步骤流(null为一个特殊的`停止`步骤)  
* 执行池  
存储每一个当前正在执行的任务与步骤,作为Metric提供和防重复执行校验  

##### 接口设计  

由主节点对外提供Rest服务  
* 任务编排,管理,重跑等WebUI对接  
* 步骤完成通知(启动下一步骤的触发)  


### ETL   

#### 采集清洗  

##### 概述  

数据采集全部转换为FTP采集  
(外围系统,负责将所有非通用方式转换为通用采集方式(手工上传=>FTP,爬取数据=>FTP 等等))  

采集过程为一个Spark作业  
核心在于通过作业参数传入采集相关元数据,在Spark作业中解析后通过一个自研的Spark-FTP外部数据源拉取数据,格式化与压缩后成为通用底层数据写入
* 自研的Spark-FTP外部数据源  
* 通过命令参数传入元数据的通道ID,解析采集元数据(FTP地址,用户名,密码,文件格式,分割符,列位等等)  
* 通过自研的Spark-FTP外部数据源,从FTP中下载为本地文件并读取为标准的订单对账底层事实表(统一Schema的DataFrame)  
* 订单对账事实表写入数仓(Parquet+Snappy)目标的HDFS路径  
* 写入完毕后更新对账元数据(通道订单数据数据块)和数仓元数据(数据块HDFS位置信息)  

##### 注意点    

* 幂等   

* 采用压缩  
* 采用Snappy压缩  
* 小文件  

#### 分析计算  

#### 结果输出  

采集系统将整体分为两个部分  
* 主流程制定正常的通用标准为: FTP或HTTP  

主流程采集与ETL的本质是一个Spark任务  
1. 启动Spark任务时将传入`元数据`中的`数据采集与清洗描述`信息,整个任务的依赖这个数据采集描述完成执行过程  
2. 自定义了一个外部数据源(FTPOrderDocumentSource),负责将读取目标FTP,将所有异构格式(Excel,CSV,TXT,RAR(Excel),RAR(CSV),RAR(TXT),Zip(Excel)......)转成一个DataFrame  
3. DataFrame清洗转换成标准的订单对账底层事实表格式  
4. 清洗结果输出到以HDFS(/通道号/天日期)(Parquet+Snappy)  

?? 始终下不了决心将这个 FTP清洗为CSV包装成Spark外部数据源,因为FTP的主被动模式需要授权,  

额外的部分  
* 压缩  
使用压缩,因为我们是小集群环境,无论是磁盘大小还是网络带宽都比较紧张,但是计算能力富裕(任务不多)  
压缩使用Snappy压缩, 因为我们的目标文件比较小,对随机读分块没有要求所以使用Snappy  
* 幂等性  
每个批次采集过程先写入Tmp,执行完毕后删除批次目录,然后mv进入,不直接对批次目录输出防止异常之后数据完全为空  
* 小文件  
df.coalesce(1)  
事实上每个通道每天的订单对账文件都不大(原始最大也就是四五百M),经过Parquet+Snappy普遍都在100M以下,所以直接输出聚合成一个就行了  
如果文件过大的话,不能coalesce(1),两个方案 根据Count估计一个大概需要coalesce的数量 或者是保留原小文件,后面通过另外的扫描程序对指定文件大小以下的小文件跑一个合并作业  

#### 分析  

分析过程的执行依赖作业调度(ScheduleSubNode触发执行一段提前写好的shell`SparkSubmit`)  
分析过程本质是一个Spark任务  
* 参数传入对账任务Id  
从元数据库中根据对账任务Id读取对账任务  
=> 读取到目标通道号(两个)+日期批次   
=>  读取元数据,确定事实表HDFS-Path  
=>  读取两个事实表HDFS   
=> 按照订单号join 提取出差异结df  

差异结果df最终会转化成两个结果  
* 差异结果  
差异结果会落盘,财务和运营人员将会从UI中下载这个差异文件(csv)进行后续核实  
* 统计结果  
差异订单数,差异总金额. 写入MySQL中,以备财务和运营查阅      

额外的部分  
* 差异DF使用缓存,因为差异DF是两段输出,使用缓存避免重复读取与计算  
* 差异DF使用序列化(Kyro,注册类型),因为前面说了我们集群计算能力富裕但网络传输紧张   
* join优化  
差异统计基于订单号join,不存在倾斜(这里浅谈如果join优化(协同分区或随机膨胀,再扯点桶表))  
双方成功&失败订单统计,事实表(订单数据)与维度表(订单状态)的join统计,典型的大小表广播  

* 

### WebUI 

#### 概述  

* AngularJS 作为UI层前端主框架,整合多个微服务系统的Rest(注意下跨域即可)  
* SSO  

#### 主要功能  
财务或运营人员  
* 查看通道对账差异报表和下载差异文件(CSV)    
* 支持在运营人员层次进行对账过程任务重跑  

技术人员   
* 技术人员随时关注 作业执行进度,ETL清洗质量(行数,记录总数,抛弃总数,补全总数等)  

