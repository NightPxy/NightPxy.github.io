---
layout: post
title:  "RDD变量&操作"
date:   2018-07-04 13:31:01 +0800
categories: spark
tag: spark
---

* content
{:toc}



# 集群环境下的变量  

Spark作为一个分布式计算框架,所有的操作都会以分布式的形式进行执行  
这体现 在执行时会由 driver端 交给 executor端去执行,在某些情况下,会与常规开发思维有所不同

## 变量与共享变量

在分布式情况下,driver变量会以数据副本的形式有交由 executor .  
这带来的问题是如果变更变量,实际上变更是变量的副本,也就是这种变更是不会传递到 driver和其它executor 的  

针对这种情况,Spark提供了两种共享变量  **广播变量(broadcast)** 和 **累加(accumulators)**  
### 广播变量(broadcast)  

#### 概述

广播变量允许将一个变量的副本发送到每个executor机器.这带来两个好处  

* 以executor节点会单位重用变量而不是作业.这样在同一个executor节点的不同作业,都可以反复使用这个变量  
* 如果变量发生变更,可以以重新广播的形式将变量推送到所有节点.  

*注意:*  
  *一般情况下,广播变量是不可变的.*  
  *但如果需要变更广播变量,变更变量本身依然是没有意义的,必须重新广播*  

#### 使用场景  

广播变量本身是会产生序列化传输的.所以只有在数据本身是可能会在多个Stage或多个Job中反复使用,使用广播变量才会变的有意义

#### 用法
```
// SparkContext.broadcast(v)进行创建,返回的是广播变量的封装器,以.value读取广播变量的的值
val broadcastVar = sc.broadcast(Array(1, 2, 3))
val v = broadcastVar.value
```

---
### 累加器(accumulators)  

#### 概述  
累加器变量仅支持累加(合并)操作,并且累加器是可以在UI中可见的  
因此累加器可以在并行计算时执行一些特殊合并计算  

注意,累加器依然遵循RDD的Lazy原则,即:  
* 累加器更新操作依然是在action中,并且在每个Job中,只会执行一次(如果Job失败重启,累加器更新不会执行)  
* 在transform中,累加器依然保持Lazy执行.(如果transform被重新执行了,则累加器会重复执行)  

#### 用法  

##### 数值类型累加器(内置)  
```
val accum = sc.longAccumulator("My Accumulator")
sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum.add(x))
println(accum.value)
```

##### 自定义类型累加器  

自定义累加器,需要继承实现 AccumulatorV2  
```
//两个泛型参数->累加的元素类型和结果类型可以不同的
class VectorAccumulatorV2 extends AccumulatorV2[MyVector, MyVector] {

private val myVector: MyVector = MyVector.createZeroVector

def reset(): Unit = { myVector.reset() }

def add(v: MyVector): Unit = { myVector.add(v) }
...
}

// 创建一个自定义的累加器类型:
val myVectorAcc = new VectorAccumulatorV2
//将这个触发器注册到SparkContext:
sc.register(myVectorAcc, "MyVectorAcc1")
```

# RDD操作  

## 概述  

RDD包含两大类操作  

* **transformation(转换)**   从一个RDD转换得到一个新的RDD  
* **action(动作)**  执行对RDD的计算并返回结果  

**action(动作) => Job**  
> 只有触发一个动作(action),才会真正产生一个执行作业来执行逻辑  

**transformation(转换) Lazy**  
> 所有RDD的转换操作都是Lazy的.也就是转换操作本身只是记录如何转换的过程.  
> 转换操作,必须在某一个动作中被需要,才会真正被执行  

## transformation(转换)  

### 转换的依赖关系

转换是从父RDD经过一些转换操作生成一个新的RDD.而RDD本身是封装有对父RDD的依赖关系的  
Spark依据RDD转换中,父RDD和子RDD分区的依赖关系,将转换的依赖分为两类 **宽依赖** 和 **窄依赖**  

#### 窄依赖  

窄依赖是指转换后,父RDD的某个或者某些分区只会被子RDD的一个分区使用.(*一对一或者多对一的关系*)  

窄依赖一般出现在map,filter等子分区沿用父分区,不会发生重分区的情况

#### 宽依赖

宽依赖是指转换后,父RDD的某一个分区,会被多个子RDD的分区共同使用.(*多对一*)  

宽依赖一般出现在groupByKey等会发生重分区的情况  

#### 宽依赖与窄依赖的对比  

一般来说,窄依赖会更加有利,这主要体现在:  

* 窄依赖允许集群节点以流水线的形式直接计算所有分区  
宽依赖则必须以shuffle的形式重新分区,在重新分区全部接收后再进行计算  

* 在某个分区发生错误需要重新计算的时候.实际是对依赖的分区进行计算(*宽窄皆同*),但是:  
窄依赖,计算的依赖的分区,利用率是100%的  
宽依赖,因为依赖分区只有一部分的数据是属于它的,但也必须重新计算整个分区.(*属于其它分区的数据白计算的*)  

### 常用操作  


#### map系操作  

**map  flatMap mapPartitions mapPartitionsWithIndex filter** 等  

map系操作的特点是:  

**真正执行是在是executor**  
> 将driver的map操作闭包进行整理,序列化传输到executor,最终在executor进行执行  

**默认不重新分区=>窄依赖**  
> 默认保留以父RDD(*dependencies.head*)的分区  

**底层转化使用MapPartitionsRDD**  
```
/**
 * An RDD that applies the provided function to every partition of the parent RDD.
 */
private[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag](
    var prev: RDD[T],
    f: (TaskContext, Int, Iterator[T]) => Iterator[U],  // (TaskContext, partition index, iterator)
    preservesPartitioning: Boolean = false)
  extends RDD[U](prev) {

  override val partitioner = if (preservesPartitioning) firstParent[T].partitioner else None

  override def getPartitions: Array[Partition] = firstParent[T].partitions

  override def compute(split: Partition, context: TaskContext): Iterator[U] =
    f(context, split.index, firstParent[T].iterator(split, context))

  override def clearDependencies() {
    super.clearDependencies()
    prev = null
  }
}
```

#### ByKey系转换  

** *groupBy* groupByKey reduceByKey  aggregateByKey sortByKey**等  
****
ByKey系操作的特点  

**重新分区=>宽依赖**  
> ByKey 都会依据key来重新分区,这样往往会产生宽依赖  
> 稍微有点特点的是 groupBy ,它默认是 HashPartitioner 来进行分区  

**真正执行是在是executor**  
> 也是将driver的分组操作闭包进行整理,序列化传输到executor,最终在executor进行执行  

**ByKey底层会转化为PairRDDFunctions[K, V]**  
```
/**
 * Extra functions available on RDDs of (key, value) pairs through an implicit conversion.
 */
class PairRDDFunctions[K, V](self: RDD[(K, V)])
    (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null)
  extends Logging with Serializable {
  ......
```

**groupByKey 与 reduceByKey 区别**  

groupByKey 与 reduceByKey ,本身是不一样的.一个是分组,一个是分组聚合.  
但如果用groupByKey来完成分组+聚合,就可能会有一定的性能问题.  

以两个最后结果完全一致的用法举例  

分组聚合  wordsRDD.reduceByKey(_ + _)   
> 因为已经知道了聚合逻辑.所以会在shuffle分组阶段提前进行一次聚合(*combine*)   
> 这个提前聚合对性能提升非常大,因为它会大幅减少map的输出  
> 比如 word的1千条记录,如果提前聚合最终map输出的只会是1条 (word,1000)  

分组+聚合 wordsRDD.groupByKey().map(t => (t._1, t._2.sum))  
> 分组和聚合是拆成两个独立的操作. 这导致分组时因为不知道聚合逻辑而无法进行map聚合.  
> 所以分组的map输出是全部输出.这会带来高IO的性能损失

