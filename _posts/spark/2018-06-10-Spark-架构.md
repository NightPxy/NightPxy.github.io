---
layout: post
title:  "Spark-架构"
date:   2018-06-10 13:31:01 +0800
categories: spark
tag: spark
---

* content
{:toc}


## Spark的集群模式  

### Spark执行流程  

Spark在集群中运行多个进程,这些进程通过driver(SparkContext运行的main方法进程)进行协调  

* 启动SparkContext所在的main方法进程,这个进程将作为driver端复杂协调整个应用程序  
* driver端启动后,会根据设置的需求的资源(内存和核),向集群管理器申请所需资源  
* 集群管理器会在集群中搜集所需的资源,搜集完毕后会将资源锁定并将资源所在节点机地址交给driver  
* driver会联系节点机,将执行任务的jar以及相关资源传给节点机(executor),并启动执行  
* 在执行过程中,executor和driver将反复通讯,executor心跳处理情况,driver协调重启  

一些重要点   

**1.每个应用程序进程属于它自己,这些进程会在整个应用程序一直存活(等待执行或正在执行)**  
* 好处  应用程序执行隔离(不同的JVM) 和 集群资源隔离(不同的Container).   
* 坏处 两个应用程序之间无法数据交换.这时可以利用外部数据存储  
* 必须注意SparkContext.stop,否则将无法释放集群资源  

**2.Spark不关心它使用在哪种集群管理器中**  
* Spark只关心driver和executors能互相通讯就行了  

**3.driver在executors生命周期中保持监听**  

### 集群调度器  

Spark虽然支持Standalone,YARN,Mesos和k8s等需要种集群调度器,但目前的主流依然是YARN模式  
在YARN模式,Spark将完全退化成客户端的形式.即Spark无需部署成集群  
集群任务的提交,将完全依赖提交到Spark环境到集群中,通过在YARN-Container中构建出Spark来执行  

所以在Spark On YARN 必须将Spark环境(jar和conf)提交到HDFS中  
PS:这一步严格来说可以省略,但是,如果没有预提交,就会在每个任务执行时自动单独打包Spark到集群中,这会极大的延长(3-5秒左右,注意是每个任务延迟3-5秒)作业申请时间,所以必须配置  

```shell
# jars 打包
zip -rj /home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/spark-2.3.1-bin-2.6.0-cdh5.7.0-jars.zip /home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/jars/*

# Spark HDFS Path
hadoop fs -mkdir -p hdfs://hadoop000:9000/spark/spark-2.3.1-bin-2.6.0-cdh5.7.0

# jars 拷贝进 Spark HDFS Path
hadoop fs -put /home/hadoop/app/spark-2.3.1-bin-2.6.0-cdh5.7.0/spark-2.3.1-bin-2.6.0-cdh5.7.0-jars.zip  hdfs://hadoop000:9000/spark/spark-2.3.1-bin-2.6.0-cdh5.7.0

# 检查
hadoop fs -ls hdfs://hadoop000:9000/spark/spark-2.3.1-bin-2.6.0-cdh5.7.0/

# 加入默认配置

spark.yarn.archive      hdfs://hadoop000:9000/spark/spark-2.3.1-bin-2.6.0-cdh5.7.0/spark-2.3.1-bin-2.6.0-cdh5.7.0-jars.zip
```


### Spark中的术语  

#### Application  

用户的整个程序,包括所有的driver和executors  

#### Application jar

用户提交Spark执行的,包括程序执行代码以及依赖资源的Jar包  

#### Driver program  

SparkContext所在main方法的执行进程  

#### Cluster manager  

集群管理器,这对Spark来说是一个外部概念,简单来说就是Spark所运行在的集群的资源管理器是什么 .比如YARN,Standalone,Mesos等等  

#### Deploy mode  

driver的执行模式.  
Spark的driver有两种执行模式 client 和 cluster  

* cluster  
driver执行在集群内部.即由集群管理器随机挑选一个Container作为driver端的承载  
* client 模式  
driver执行在集群之外,即在提交任务的客户机上启动一个进程作为driver端的承载  

#### Worker node  

Executor所在的集群节点  

#### Executor  

在工作节点上运行的进程.负责运行任务线程和存储任务数据.  

#### Task  

发送给每一个executor去执行的一个工作单元  

#### Job  

在Task之上运行的Spark Action产生的任务,包含Action,已经Action之前的Transform的执行单元  

#### Stage  

这是Spark的内部执行单元.  
每个Job最终都分为一个或多个Stage来最终执行  

### Deploy mode对比  

client 的优势和劣势都是 driver端是我们可以预测和定义.而cluster模式,因为driver是在集群之内,所以是由集群管理器随机挑选一个作为driver端  

* client模式单点问题  
因为大量driver端都集中在某一台机器上(提交任务的那台),如果该机器出现问题,很容易造成大面积的Spark任务失败.因为executors必须随时与driver保持通讯,即driver必须存活  
* client模式可能出现网络瓶颈  
因为executors必须随时与driver保持通讯,即该台机器必须于大量的executor节点进行网络传输,集中在集群之外的某一台,可能会造成该台机器的网络瓶颈  
* clinet模式的执行日志始终在指定机器,而不是随机节点  
这可能是client模式的唯一好处了  



