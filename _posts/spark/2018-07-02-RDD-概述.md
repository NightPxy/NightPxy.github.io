---
layout: post
title:  "Spark-RDD概述"
date:   2018-07-02 13:31:01 +0800
categories: spark
tag: spark
---

* content
{:toc}


## 什么是RDD
**Resilient Distributed Dataset(RDD)** 弹性的分布式数据集  

### 分布式数据集
是指数据会以 **分区(Partition)** 的形式分布在多个节点上  

而Partition会最终决定Spark的Task数量(*Spark会为每一个分区创建一个Task*)  

### 弹性
对物理数据集进行抽象封装.  

以此在Spark计算过程中带来各种重试和恢复机制,内存和磁盘的切换,高自由度的分区函数等  

* Hadoop的MapReduce,是基于物理数据集的处理.  
从磁盘中加载数据,操作数据并最终写入磁盘.这导致复用中间结果的IO成本太高  
* RDD将对物理数据集进行抽象封装.这种封装为RDD带来两个强大的功能

**1. 封装自动的内存与磁盘切换**
* 保留基于分布式文件存储的高容错性  
CheckPoint 机制将RDD基于分布式文件系统的多副本存储
* 内存方式的高效复用中间结果  
Persist 机制将RDD缓存至内存,并允许在缓存内存自动溢出到磁盘

**2. 封装多个RDD之间的依赖关系**
抽象RDD的依赖关系,这样在某一个RDD出现问题时可以根据依赖重新生成该RDD.这就为RDD带来非常强大的弹性恢复特性:
* 基于依赖链的容错.(第N个节点出错,会自动从N-1个节点重新计算恢复) 
* RDD的容错性和复用中间结果的低廉成本,还可以为整个Spark的计算过程带来自由的重算容错机制.比如Task级别的重算容错,Stage级别的重算容错等等

**3. 将各种数据源提升到一个抽象的层面来统一解决**
* Spark本身提供了诸如文件,JDBC,Streaming还有kafka等等各种数据源.
将异构数据都统一封装成RDD,就可以非常弹性的统一解决,甚至直接合并来自不同数据源的数据

## RDD的五大特性

熟知RDD有五大特性,其中分区性,函数性,依赖血缘关系是RDD的三个基本特性.分区策略和就近原则是可选特性(这里的可选是指Spark本身提供了默认实现,比如分区策略默认使用Hash分区,就近原则默认使用Block所在节点计算等,但提供了API以供开发人员干预这些过程).

1. **分区性**: RDD是由多个分区(partition)组成的一个集合
>A list of partitions
2. **函数性**:对RDD的每一个计算,等于对这个RDD的每一个分区执行一个计算
>A function for computing each split
3. **依赖血缘关系**:每一个RDD都会记录它的父RDD的依赖
>A list of dependencies on other RDDs
4. **分区策略**:如果RDD里存放的是Key-Value的形式.则可以传入一个自定义的分区函数进行分区
>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
5. **就近原则**:计算会尽可能的放入split所在的节点中(也就是Block所在的节点)
>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)
