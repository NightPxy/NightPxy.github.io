---
layout: post
title:  "Spark-创建"
date:   2018-07-03 13:31:01 +0800
categories: spark
tag: spark
---

* content
{:toc}


## 从本地集合创建  
```
//声明一个本地集合
val data = Array(1, 2, 3, 4, 5)
val distData = sc.parallelize(data)

/**
*分布式数据集,有一个重要参数就是数据分片数量(Spark会在每一个分片跑一个task)
*本地集合创建,默认情况,Spark会根据你的集群数量自动设置分片数
*也可以手动指定这个数据集的分片(第二个参数)
*/
//val distData = sc.parallelize(data, 10)

//一旦分布式数据集创建完毕,这个数据集就可以并行的被操作
distData.reduce((a, b) => a + b)
```

## 从一个外部的存储系统中创建  
* 这里外部系统,指的是任何Hadoop(InputFormat)支持的存储系统.  
  比如本地文本文件,HDFS,HBase,S3等等  

* 对于外部文件,Spark会按照128M(HDFS默认),来进行分区  
  这里可以手动设置分区.但需要注意的是**手动设置的分区数必须要大于默认分区数**  
  即只能增大分区,不能减小分区.(*减小分区无效*)

  

### textFile  
```
val distFile = sc.textFile("hdfs://hadoop000:9000/xxx/data.txt")
            
/**
*这里纯粹的本地文件是不推荐的
*因为这个文件访问是针对每一个Worker都要是能访问的
*  换言之,如果是本地文件,则必须保证每一个Worker的本地都有一份这个文件
*/
//val distFile = sc.textFile("/data.txt")
	
/**
*Spark支持文件目录,压缩文件,或者通配符等
*/
//val distFile = sc.textFile("hdfs://hadoop000:9000/xxx/*.gz")
	
/**
*手动调整分区
*/
//val distFile = sc.textFile("hdfs://hadoop000:9000/xxx/*.txt",10)
	
distFile.map(s => s.length).reduce((a, b) => a + b)
```

### wholeTextFiles  
wholeTextFiles是用来读取某个文件目录下的多个小文件的  
与textFile的区别是:  
  textFile 以行断符为分割.一个记录就是一行  
  wholeTextFiles 是以文件为分割,一个记录就是一个文件内的全部内容  
wholeTextFiles的默认情况,可能导致分区数太小.这时可以手动设置调高分区数  

### sequenceFile[K, V]  
将数据集中的元素以Hadoop Sequence文件的形式保存到指定的本地文件系统、HDFS或其它Hadoop支持的文件系统中  
该操作只支持对实现了Hadoop的Writable接口的键值对RDD进行操作  
在Scala中，还支持隐式转换为Writable的类型（Spark包括了基本类型的转换，例如Int、Double、String等等)  

### hadoopRDD  
对于其它的Hadoop InputFormats,可以hadoopRDD读取  
传入JobConf,input format class,key class and value class(与MapReduce任务设置相同),就可以直接以MapReduce作为输入源进行读取  

### saveAsObjectFile  
将数据集中的元素以简单的Java序列化的格式写入指定的路径。这些保存该数据的文件，可以使用SparkContext.objectFile()进行加载
