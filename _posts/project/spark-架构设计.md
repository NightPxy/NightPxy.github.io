
* content
{:toc}


## 问题域  

我们需要一个分布式计算框架  
从老祖宗Hadoop那里传下来的分布式计算的核心思想是分治,也就是分片计算,这个分布式计算框架也照这个思路  

## 建模描述  

### 核心建模  

#### 集中式  

##### 分片计算  

解决这个问题,第一步是建模,定义出编程模型来进行描述,这个编程模型就`RDD`  

首先模型`RDD`,是以`分片计算`为根基,`分片计算`三要素:`怎么分`,`各分区数据`,`计算逻辑`  
所以`RDD`最核心的部分是  
* partitioner 分区器,用以描述怎么分片  
* getPartitions 各个分区的数据  
* compute(partition) 计算逻辑   

##### 计算迭代  

作为一个分布式计算框架,计算是核心,也就是我们希望这方面的能力越强越好  
这方面的MR给我们做出一个不太好的设计,就是粗粒度计算(提供一个超级大算子,使用者自己实现它(在我的理解中MR,Map是分片算法,计算算子其实只有一个,就是Reduce算子))  
算子的设计,以细粒度更好,从算子的细粒度切分,我们可以抽离出通用的与业务无关的部分作为通用算子,比如`map`,`groupByKey`,`reduceByKey`等等,让使用者通过通用算子的组合迭代,来完成复杂的计算  
所以`RDD`继续抽象  
* 各种通用算子  
* 迭代链,这里用 `Array[computeFuns]`代替  

##### 容错能力  

作为一个分布式框架,容错是必须的  
常见的容错有两种  
* 数据容错  
数据的容错,一般的解决方案是多副本  
就是数据防止落地一个或多个副本,如果一个数据有问题,从它的副本或者副本集还原  
* 过程容错  
过程的容错,一般的解决方案是幂等重演  
这需要过程的执行日志记录,如果一个过程有问题,就根据执行日志重演一次(这中间注意幂等)  

`RDD`继续抽象  
* 过程容错,执行日志记录是天然存在的,就是我们的迭代链  
* 数据容错  

##### 优化  

###### 延迟分区数据  

因为迭代链的关系,每迭代计算一次就产生一份数据,无限膨胀下来根本行不通的  
所以第一个要做的是延迟分区数据  
所以`getPartitions`,分区数据内只能是如何获取分区数据的描述而不能是数据本身  

###### 迭代计算效率  

迭代计算因为延迟加载分区数据的关系,最终会落到数据源上去,这在很大程度上是没有必要的甚至性能非常低下的  
所以会引入一个在迭代过程中的数据缓存概念,就是到这里先把数据缓存一下,以后追溯到这里就可以直接使用缓存数据而无需继续向上迭代了  
说到`数据缓存`,这里就是惯常手段了  
* 缓存当然是内存最好,常规思路就来了   
缓存如何释放,这个一般都是`LRU`  
内存缓存必须设定上限防止OOM  
以什么形式存储 `序列化`缩小内存占用,利于释放,但增加序列化与反序列化的计算成本  
* 内存总有放不下的时候,这时丢弃还是写磁盘  

这些部分都是计算框架本身无法决定的,这个简单,交由用户决定  
所以抽象出数据缓存级别 `memory`,`memoryAndDisk`,,`memorySer`,交由用户决定  

其次,这一套缓存机制它是一种与业务无关的计算过程机制,更希望由框架本身提供而对用户透明  
`RDD`在模板中定义`iterator`方法作为真正的对外使用  
`iterator`内部封装如果这个RDD被缓存则优先从缓存中直接读取,如果没有才会从执行`compute`计算    

###### 总结  

到这里,这个`RDD`已经能实现我们的很多功能了  
* 能够对数据分区,并进行计算  
* 能够支持复杂的计算,这种复杂计算是以迭代通用算子的形式完成的  
* 迭代的中间数据支持基于内存或者基于磁盘  
* 有基本的`数据容错`和`过程容错`能力  

但到这里为止,这还是个集中式的计算抽象,说白了还是单机版的,接下来把它分布式化  

#### 分布式  

##### 分布式架构  

既然是分布式,不管是不是多机器,至少是多个JVM执行,这里每一个JVM暂称为`executor`   
所以这个分布式计算,就类似跑了多个集中式单机计算来一起执行  
这里有个很关键的问题,集中式计算中通过一个分区器完成分片然后各自计算,但多个集中单机式之间彼此很难协调,这样的分片计算要么有重复,要么有漏分区  
这个彼此协调的工作非常复杂非常难以完成,这个时候老祖宗提供了一个解决方案:`主从架构`  
`主从架构`:既然很难协调那就不协调了,找一个主管事的负责分配完成,大家只负责干自己的  

所以这个分布式载体中`executor`会加以区分,`executor`和特殊的`driver`  
`主从架构`的另一个需要考虑的是是否需要`去中心化`  
`去中心化`不是无中心化,而是这个中心是可以随时被替代的,一般来说`去中心化`需要引入`ZK`之类的外部存活和选举机制让中心流动  
`中心化`的优势也很明显,固定中心可以极大的简化复杂度,并且中心固定化,就可以让中心承担更多的责任,比如元数据存储等等,这个之后说  

所以最终决定分布式架构为 `driver`+`executor`的`中心化`架构  

##### 分布式通讯  

无论是分配,还是心跳等等,分布式节点肯定是需要通讯的,分布式通讯一般有两个方案  
* 端到端  
* 广播&多播 非定点通讯  

这里必然是端到端的,因为driver是主分配调度,为了最大限度的提升driver的分配调度的控制力度,控制到端到端是最好最强有力的方式  
端到端通信网络,常规思路又来了  
* 主节点启动  
* 从节点启动时传入主节点位置,获得从节点对主节点的RPC引用  
从节点启动后注册到主节点,主节点获得从节点的RPC引用     
* 从节点持续心跳,主节点以此感知从节点的存活情况  

这样主从之间互相了解,完成主从的消息网络

##### 分布式化  

决定了`driver`+`executor`的`中心化`分布式架构,接下来是对单机版RDD进行分布式化  

* 计算的分解  
* 数据混洗  
* 分布式计算的执行  
由`driver`提供计算迭代链(也就是算子函数)的反射点(Jar包,类,函数以及签名), 通过上面的通讯网络发送给`executor`,`executor`照着这个迭代链反射点反射执行  
* 



